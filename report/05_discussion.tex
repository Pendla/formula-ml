\chapter{General Discussion}


\section{The NEAT mechanism} \label{discussion:neat_mechanism}
%  The topics noted with X are things I think can be included in the next version of the text

%   - Neat train to fulfil a declared goal, it is not a description on how to do it
%     - The fitness value say how good a genome is
%     - It do not base its decisions upon previous path of progress
% x - Analysis: How entangled is a neural network? Can one part evolve safely without disrupting other parts? Is it possible to evolve safely? Catastrophic forgetting?
% X - Ability to manage multiple goals? Can it achieve one or several tasks at the same time?
%   - How well do the greedy approach work?
%   - Would it be better to denote fitness based on behavioural heuristics rather than result based heuristics?

%   - What are the properties for getting stuck or to develop innovation? 
%   - Do it exist certain statistical improbabilities or unavoidableness?
%   - How will the stochastic parameters in neat influence the performance? 
%   - We see that many early solutions have very few nodes, solutions that have gone through long training have many. How much do the performance really differ? 
%   - Often only few of the input values are covered.
%

%   Wild ideas/suggestion:
% X - An image Daniel presented, showed during a long training, it was rare that networks made it around the track. After it succeeded to improve the results, or get around the track, the performance was always lost shortly after (?). Does this show that the neat algorithm in the setup we used it, is not efficient? Is avoiding local minima, too aggressive?
% X - In the neat paper, the authors stated that the training often got stuck for a while. Then, when some kind of archetype structure was found, the performance could then quickly increase as good networks was derived from the archetypes. It is not far fetched that the two steps in the process have different requirements. Maybe archetype topologies are smaller and faster to sieve. The elaboration of an archetype maybe requires more processing, as it might involve larger structures and thus larger need for tuning and compensation when evolving. Suggestion: Change the stagnation limit (and possibly other parameters) as the number of generations increases, or when certain properties relating to the role a species/genome hold. It might not be possible to find a sweet spot that works for all situations. 
%   - What has progressed after a leading genome has stagnated and died?
% X - What size of a network is beneficial for a network to probably to be an archetype? small? connect to all input values, or cover all categories of inputs? After a successful 
% X - How general will this approach be? If it is successful, will it only be it for problems that resemble characteristics from the racing problems? If, so what aspects? If not, what characteristics do it solve/target?

As previously described WHERE WHERE REFERENCE WHERE WHERE, NEAT is detached from the the actual problem it solves. It does not do any particular analysis of the problem and very little analysis of the results it collects along the way. The only feedback that it receives is the fitness value. One might question how useful this is when solving complex problems.

The NEAT algorithm is not smart. It does not know or understand what it is currently performing well at nor what it is lacking, therefore it has no possibility of applying directed modifications. The progression of the NEAT algorithm depends on the fact that a better solution is within reach of one that is currently in the active genome pool.

NEAT progressively builds up networks. Even if many different networks are evolving concurrently, each of them evolves in small steps. The number of changes for a particular network are rather limited for a single generation. In order for NEAT to produce a successful species, it is required that the mutation that will give the highest contribution towards the end goal also give enough fitness to survive. If this is not the case, the species and individual genomes will be discarded even if they could have produced better results in the future. 

If a network performs bad in comparison to the others in the same species, or if a species perform too bad in comparison to the other species they will be discarded\cite{stanley:neat}. Also if a species do not improve for a specified amount of generations, so called stagnation, it will also be discarded. A third aspect is that worse performing species will have a smaller amount of individuals in its species. Therefore it exists a rather well defined range for where individuals and new innovations survive. It is necessary not to fall in fitness, as well as improving before stagnation or breed a new species exploring a similar approach.

When networks evolve by changing weights and introducing new topology, it is important that it remains functioning, with respect to the task that it previously succeeded with. If it does not, it might fall out of the favourable range and get discarded. 

% [TODO: Write about catastrophic forgetting?]
%This kind of problematic has been called catastrophic forgetting\cite{gomez:CoSyNE} NOT REALLY WHAT THEY MEANT, THEY WERE TALKING ABOUT LEAVING A TASK AND TRAINING FOR ANOTHER, THEN RETURNING TO THE PREVIOUSLY LEARNED TASK

It therefore seems important that performance and complexity are allowed to increase gradually, without having to perform large jumps. If the mutation that is required is large enough, NEAT may not be able to find the correct path to reach it. Thus if NEAT is successful, a path was found that only required small progressions from start to finish.

[TODO: CHECK RESULT FOR THIS DISCUSSION] One example of this may be the steer and speed control. It first train to complete the lap, but then, when a more complex analysis are required in order to make it possible to control speed efficiently, it may be somewhat locked in to the first strategy. It might not exist a gradual path that manages all the curves and it will temporary have a significantly decreased fitness. This lead to a problematic on whether it is better to complete the whole track, with a limited quality of behaviours, or only a part of it with better general characteristics.

The difficulty of gradual progression may be particularly problematic when archiving behaviour with several in-excludable components. If several aspects are required to progress simultaneously, and any of them are missing, it might cause a failure. [TODO: Make sure this example is actually correct.] One example might be steer and speed control. If a network changed to drive at a higher speed, but does not change the position of the car accordingly, it will likely cause the car to crash. This fact might be one of the reasons as to why some experiments do not perform in optimal ways.

%WHAT happens when the leading species die? HOW much progress are handed over to other species?

% CAN it improve on multiple goals, or only once at a time? How do it manage to learn behaviour with in-exclusable components?
% It is possible to add multiple edges in a few number of generations, but it is unlikely that they are the relevant ones and that they are well tuned. e.g. gas and brake. Kanske ofta utveckla en sak i taget.

NEAT has proven itself to be useful. We can see in some experiments, e.g. shortest path, that NEAT is able to find beneficial behaviours that often resemble target characteristics. But it seldom learn all of the smaller details. We have never observed that it solves everything. Together with the discussion above, we are doubtful that NEAT will ever be able to find the perfect solution. However, while the discussion deducted from the key mechanics of NEAT and the results from our study, does not prove that NEAT cannot find a complete and all-round optimal solution for the kind of problem we investigated, it highlights a structural weakness that cannot be denied.





\section{Problem modelling}
% Possible to add:
% - distance to middle, sometimes proved bad (?!?!?) in the steer and speed control experiment. If a value is easily applicable to the network, it is good. But when considering distance to middle in steer and speed, it might be too easy, and therefore a longer jump for it to learn to analyse it more properly. This is based on that the behaviour is that it drives in the middle, and that the specific data point is used to accomplish that behaviour. The middle behaviour is not present in the constant speed experiments when it is forced to drive fast and therefore has to do good positioning. 

% - However, if data is pre processed to extract a specific kind of property of the data other information might be lost. In order to not loose necessary information a greater number of inputs might be required, which may in itself increase the complexity. Example: Curve data points as today, but with values on how sharp the curve inside the segment is, then two floats are needed instead of one and one might think it is better to simply double the resolution of points instead.

% - In context of neat and curvature data: Fixed position is a compact representation. Information lay in the index/topology. In order to interpret distance to an observed object a structure in topology is required. If the exact distance for which some action should need to change, the topology need to change, which is a difficult task. Index based distances for actions maybe require an abstraction layer on top of the input in order to make them parametrised. Could an interpreted object input model improve the adjustment capabilities, example touple with distance to the curve and its properties? Then some more of the interpretation lie in the weights and not in the topology itself. modifying the topology is expensive and probably in most cases statistically impossible.

In order to reach the best results, it is important to use NEAT to its strengths and avoid its weaknesses. One aspect is how the problem is modelled for the neural network, what data is it provided with and how is the result interpreted.

The process of taking decisions based on data may be divided into three steps: (1) Process and interpret data and (2) making a decisions. It is conceived that it is beneficial to focus the responsibility of the networks as much as possible, and let it focus manly at the latter two. 

%SHOW WITH GRID DATA EXPERIMENT??

A way to analyse different sets of data is by the information they contain and how relevant it is to the problem. If the data contain noise the training have a more difficult task to learn.

Concerning the track, it could get data straight from the representation model used by the simulator, vector points in triangles. However, it would have to learn how they relate to each other and what the relevance of them are is. The curvature data describe approximately the same thing, with a much lower amount of data. The exact coordinates of the triangles are not relevant, but how they relate to the car and how it affects the race line. 

To some level, the developer need to do the analysis of what is relevant and good information. If the data get to compact or hard to calculate with, it might get difficult to learn to use it, based on the discussion about multiple goals section \ref{discussion:neat_mechanism}.

To some extent it might also be important that the values are easily calculable, to make it simpler to find a usage. Several of the input data that was repeatedly used did not provide new information, but was other representations of data already provided. Distance to middle, right and left edge are in a sense analogous as the track in our experiments was uniform in width. The curvature segment data are simply the summation of some of the other data points.

We do often observe that both variants, distances and curvature and their transformations, are used at the same time. It suggest that both variants was usable in the training process, at least at some point. They could have been calculated from the other form of data, but that would have required a more complex network.



%Generality of results
%- How stable and general is it? Can it manage the same curve in different situations? Stability for unpredicted factors?
%- Over training. What do it mean, is it a risk and can it be avoided?

% Dicuss generality of the discovered behaviours
% Relate to the experiments, especially mirror track. 
\section{Generality of results}

One goal of the project was to find general behaviours, which means that the system should learn to drive on many different circuits instead of memorising a script of how to drive on a specific track. We showed in the mirrored track experiment that ..!!!!.. . Though that is not the only aspect that affect the generality of the solution. 

There is also the possibility of over-fitting the neural networks to the specifics of the training environment. This means that a behaviour is found that by chance works well on the training circuit, but not in general. For example if the system learns to distinguish specific locations on the track and take decisions based on which segment the car is located in instead of the track shape.




\section{Neat usability}
%   - How well do it perform with not knowing how things work?
%   - For how large and complex problems can it perform?
% X - In our experiments it controlled the car at a low level, can improvements be made if it is modelled differently by the programmer? Added abstractions? More supplemented by other parts of a solution? Several single purpose networks in modules?
%   - How much domain knowledge is required to make it ok/very good?
%     - Degree of knowledge?
%     - Type of knowledge? Examples: Structural knowledge provided by the programmer, example data, experience
%     - Worth noting: excluding something unimportant also prove a kind of knowledge
%     - Might be good in a situation where one know what behaviour is wanted, but not how to accomplish it.
%   - Find a fitness function that gives smooth progress to the maximum, with as few local maximums as possible. 

The results presented contain both aspects of success and aspects failure. It managed the control task to some degree, but it appear that the usage of neat presented uncertainly can solve problems with too much the complexity. As discussed in section \ref{discussion:neat_mechanism}, the performance is limited when steps in which it is required to progress is too large.

As described in SECTION SECTION SECTION, NEAT is not an algorithm designed to solve a particular problem. Instead it produces artificial neural networks through neuroevolution. The process is solely controlled by the fitness value. As such, NEAT do not do any particular analysis of the actual problem. 

If NEAT is sufficient to solve a problem, the simplicity make the algorithm easily implementable as few domain specific processes are needed. It also means that the developer are not required to have as deep domain knowledge, as the algorithm managed to solve the problem on its own. It usually require more effort and advanced knowledge in order to solve a problem, than it is to know about the problem.



\section{Comparison to other algorithms and concepts}
- How do it stand to other algorithms?

- Compare degree and type of domain knowledge needed? Example of types: Structural knowledge provided by the programmer, example data, experience



